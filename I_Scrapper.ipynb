{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "I-a-Scrapper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFpKPmeOUPS5"
      },
      "source": [
        "# Scrapper de sites\n",
        "\n",
        "> Récupère le contenu de chacun des URL classé par typologie (variable ndf dans le code), chargé depuis le fichier Sites.json.\n",
        "\n",
        "> Stocke le résultat dans ndf qui conditionne la variable \"fichier de sortie\". Celle-ci peut être adaptée pour pointer sur une zone correcte de votre drive dans la zone \"Personnalisation\" dénotée ci-dessous. Les données des sites collectés sont stockées au format pickle dans le dossier /OUT/ContenusSites classé par typologie selon Sites.json.\n",
        "\n",
        "> La variable de sortie est de type dictionnaire pour conserver les url sources (flexibilité pour pouvoir corriger). Cf. commentaires infra. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0A5eMDAyoOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "076dcc68-9f64-4828-c785-0f561f3f56db"
      },
      "source": [
        "# Il faut initialiser le dossier à partir de GitHub \n",
        "# cette action est à faire une seule fois lors de la première utilisation\n",
        "!git clone https://github.com/Patent2net/DistorsionCom.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DistorsionCom'...\n",
            "remote: Enumerating objects: 198, done.\u001b[K\n",
            "remote: Counting objects: 100% (198/198), done.\u001b[K\n",
            "remote: Compressing objects: 100% (160/160), done.\u001b[K\n",
            "remote: Total 198 (delta 60), reused 172 (delta 34), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (198/198), 13.96 MiB | 14.94 MiB/s, done.\n",
            "Resolving deltas: 100% (60/60), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra410KOWyoyR"
      },
      "source": [
        "stockageEntree = \"/content/DistorsionCom/OUT\"\n",
        "stockageSortie = \"/content/DistorsionCom/OUT\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txv0Pk7P_xbw"
      },
      "source": [
        "\n",
        "---\n",
        "Personnalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rntapC_RCnd"
      },
      "source": [
        "> Ces cellules permettent de réaliser les traitements à partir de son propre espace de stockage. A n'exécuter que dans ce cas en adaptant les dossiers d'entrée et sortie. NE PAS EXECUTER SAUF A PERSONNALISER LES TRAITEMENTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr3_QQJq_veT",
        "outputId": "f4fed0ea-23de-42cf-b99d-5cdc9b452730"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ozLFiEG_w5H"
      },
      "source": [
        "# Récupérer les sorties sur son drive (créer un dossier \"OUT\")\n",
        "stockageSortie = \"/content/drive/MyDrive/OUT\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpnXg8eWHorz"
      },
      "source": [
        "# Récupérer les entrées sur son drive\n",
        "stockageEntree = \"/content/drive/MyDrive/OUT\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbKLKuPg_0jY"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvC9l2YvRmP9"
      },
      "source": [
        "import requests, re, pickle\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYD-cWsJzFMn"
      },
      "source": [
        "import json\n",
        "with open(stockageEntree + '/Ressources/Sites.json', 'r', encoding='utf-8') as f:\n",
        "    CatSites = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zxsE3AeBrCd"
      },
      "source": [
        "# Première boucle pour lever les problèmes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhM4fmUohayD"
      },
      "source": [
        "> La dynamique du web fait que certains sites peuvent ne pas être accessibles, ne pas répondre à un instant t, ou faire planter le collecteur. Ce qui suit teste chaque URL et construit la variable BadUrl avec les urls en erreur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZUYVvJjWKIW"
      },
      "source": [
        "BadUrl = dict()\n",
        "Done = []\n",
        "for ndc in CatSites.keys():\n",
        "  if ndc not in BadUrl.keys() and ndc not in Done:\n",
        "    BadUrl[ndc] = []\n",
        "    print(ndc)\n",
        "    for url in CatSites[ndc][0]: \n",
        "      try:\n",
        "        webpage = requests.get(url)\n",
        "      except:\n",
        "        print(\"bad\", url)\n",
        "        BadUrl[ndc].append(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUPIAATTEjiR"
      },
      "source": [
        "BadUrl [ndc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNQngfXfRIGw"
      },
      "source": [
        "done = []  \n",
        "# pourrait être initialisé par un os.listdir() sur le répertoire de sortie\n",
        "     \n",
        "for ndc in CatSites.keys():\n",
        "  if ndc not in BadUrl.keys():\n",
        "    BadUrl[ndc] = []\n",
        "  if ndc not in done:\n",
        "    soup = dict() # changement de type de données pour conserver l'URL source\n",
        "  # récupère l'URL d'un site web et enregistre la page web\n",
        "    for url in CatSites[ndc][0]: \n",
        "      if url not in BadUrl[ndc]:\n",
        "        webpage = requests.get(url)\n",
        "      else:\n",
        "        pass\n",
        "  # récupère le contenu de la page web à l'aide de BeautifulSoup\n",
        "    #soup.append(BeautifulSoup(webpage.content, \"html.parser\"))\n",
        "      tempoSoup = BeautifulSoup(webpage.content, \"html.parser\")\n",
        "      if tempoSoup.title is not None:\n",
        "        titre = tempoSoup.title.text \n",
        "      else:\n",
        "        titre = \"\"\n",
        "\n",
        "      texte = [x.get_text() for x in tempoSoup.find_all('p')]\n",
        "      soup[url] = titre + '\\n' + \"\\n\".join(texte)\n",
        "      # le contenu est dans le \"casier\" nommé par l'url\n",
        "    # nettoyage\n",
        "    for cle in soup.keys():\n",
        "    # soup [cle] = re.sub('^a-zA-Z0-9àâäèéêëîïôœùûüÿçÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ', ' ', soup [cle])\n",
        "      re.sub('^a-zA-ZàâäèéêëîïôœùûüÿçÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ', ' ', soup [cle])\n",
        "      soup[cle] = soup[cle].replace(\"\\xa0\", \" \")\n",
        "      soup[cle] = soup[cle].replace(\"\\n\", \" \")\n",
        "      soup[cle] = soup[cle].replace(\"’\", \"'\")\n",
        "      soup[cle] = soup[cle].translate('utf8')\n",
        "      dictionary = {\"\\\\\": \"\"}\n",
        "      transtable= soup[cle].maketrans(dictionary)\n",
        "      soup[cle] = soup[cle].translate(transtable)\n",
        "      soup[cle] = str(soup[cle])\n",
        "    \n",
        "    # c'est pas tip top. Certains caractères restent. Variable selon les sites. \n",
        "\n",
        "    # sauvegarde sur Google Drive\n",
        "    fichierDeSortie = stockageSortie + '/OUT/ContenusSites/' +CatSites [ndc][1] +'.pkl'\n",
        "    with open (fichierDeSortie, 'wb') as  fictemp: #on met tous les contenus dans un pickle\n",
        "      pickle.dump(soup, fictemp)\n",
        "    done .append (ndc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IkPL8C9V6ix"
      },
      "source": [
        "# Nettoyage\n",
        "\n",
        "1. Il semble que le texte récupéré contienne de nombreux échappements d'apostrophes : `\\'`\n",
        "qui sont un \"encodage\" informatique particulier. Une substitution pour les remplacer par un apostrophe simple évitera que spacy s'embrouille.\n",
        "\n",
        "2. BarUrl contient tous les url plantés rangés par catégorie. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odkUDVgOKdKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a1eeb11-a322-4dc2-82f8-e0558f758b80"
      },
      "source": [
        "import pprint as pp \n",
        "\n",
        "pp.pprint (BadUrl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Hebergements': [],\n",
            " 'Services': ['http://group-indigo.com', 'http://secretrip.pro'],\n",
            " 'gouv': [],\n",
            " 'locations': [],\n",
            " 'mairies': [],\n",
            " 'oTourism': [],\n",
            " 'orga': [],\n",
            " 'parcs': ['http://www.cevennes-parcnational.fr/fr',\n",
            "           'http://www.mercantour-parcnational.fr/fr',\n",
            "           'http://www.reunion-parcnational.fr/fr'],\n",
            " 'restauration': []}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQm5mqYgVeGj"
      },
      "source": [
        "## Commentaires et exemple de ce qui est récupéré\n",
        "\n",
        "\n",
        "*   les clés du dictionnaire de sortie sont les url\n",
        "*   Le contenu du dictionnaire pour une clé donne le texte récupéré\n",
        "* L'équivalent avec le précédent format (toutes les pages dans un texte) s'obtient facilement\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px_PttxXTsvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3e21c31-6879-497c-80f7-e3974d3dc6ad"
      },
      "source": [
        "soup.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['http://la-seyne.fr', 'http://le-pradet.fr', 'http://isere.fr', 'http://hyeres.fr', 'http://ville-lagarde.fr', 'http://toulon.fr', 'http://carqueiranne.fr', 'http://villedelacrau.fr'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gn7V-QWTyQV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "442fe193-fe2c-4050-cd41-fe67f5d845bd"
      },
      "source": [
        "soup ['http://la-seyne.fr']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"La Seyne.fr - Site officiel de la ville de La Seyne-sur-Mer Accueil   A Decrease font size. A Reset font size. A Increase font size.          Suivez l'actualité seynoise : portraits, dossiers, actions, événements… Toute l'information au plus près de chez vous.  A l'occasion de la Semaine européenne pour l'emploi des personnes handicapées (SEEPH), et sous l'impulsion de Valérie Guittienne, élue en charge de l'inclusion des personnes en situation de handicap, une matinée « Rencontres entreprises » s'est tenue le 16 novembre à Saint-Elme. Découvrez tous les événements culturels, sportifs, associatifs sur l'agenda officiel de la ville. Côté mer, vous découvrirez le célèbre rocher des “Deux Frères” depuis la plus grande plage de La Seyne “Les Sablettes”, ou bien vous plongerez dans ses eaux cristallines en vous initiant à la plongée sous-marine en toute sécurité ! Côté forêt, ce sont des paysages grandioses et préservés qui s'offriront à vous. La Corniche Merveilleuse qui vous permettra d'accéder au sommet du massif du Cap Sicié d'où vous pourrez admirer un panorama exceptionnel sur le littoral et les îles.  Côté histoire, la ville vous propose sa diversité et sa richesse avec un coeur de ville provençal qui témoigne de plus de 500 ans de construction navale. Laissez-vous surprendre par Tamaris, luxueuse station de saison créée par Michel Pacha mais aussi par les fortifications militaires mythiques comme le fort Balaguier et ou encore le fort Napoléon. 20 quai Saturnin FabreCS 6022683507 La Seyne sur mer HORAIRES D'OUVERTURE Lundi – Jeudi : 8h à 17h30 Vendredi : 8h à 16h30  HORAIRES D'OUVERTURE Lundi – Jeudi : 8h à 17h30 Vendredi : 8h à 16h30\""
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}